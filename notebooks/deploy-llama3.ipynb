{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Llama 3 on Amazon SageMaker\n",
    "\n",
    "Earlier today Meta released [Llama 3](https://huggingface.co/blog/llama3), the next iteration of the open-access Llama family. Llama 3 comes in two sizes: [8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) for efficient deployment and development on consumer-size GPU, and [70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-instruct) for large-scale AI native applications. Both come in base and instruction-tuned variants. In addition to the 4 models, a new version of Llama Guard was fine-tuned on Llama 3 8B and is released as Llama Guard 2 (safety fine-tune).\n",
    "\n",
    "In this blog you will learn how to deploy [meta-llama/Meta-Llama-3-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) model to Amazon SageMaker. We are going to use the Hugging Face LLM DLC is a purpose-built Inference Container to easily deploy LLMs in a secure and managed environment. The DLC is powered by [Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference) a scalelable, optimized solution for deploying and serving Large Language Models (LLMs). The Blog post also includes Hardware requirements for the different model sizes. \n",
    "\n",
    "In the blog will cover how to:\n",
    "1. [Setup development environment](#1-setup-development-environment)\n",
    "2. [Hardware requirements](#2-hardware-requirements)\n",
    "3. [Deploy Llama 3 70b to Amazon SageMaker](#3-deploy-llama-2-70b-to-amazon-sagemaker)\n",
    "4. [Test and chat with the model](#4-test-and-chat-with-the-model)\n",
    "5. [Benchmark llama 3 70B](#5-benchmark-llama-2-70b)\n",
    "6. [Clean up](#6-clean-up)\n",
    "\n",
    "Lets get started!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "We are going to use the `sagemaker` python SDK to deploy Mixtral to Amazon SageMaker. We need to make sure to have an AWS account configured and the `sagemaker` python SDK installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.216.0\" --upgrade --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name philippschmid to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::558105141721:role/sagemaker_execution_role\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to deploying regular Hugging Face models we first need to retrieve the container uri and provide it to our `HuggingFaceModel` model class with a `image_uri` pointing to the image. To retrieve the new Hugging Face LLM DLC in Amazon SageMaker, we can use the `get_huggingface_llm_image_uri` method provided by the `sagemaker` SDK. This method allows us to retrieve the URI for the desired Hugging Face LLM DLC based on the specified `backend`, `session`, `region`, and `version`. You can find the available versions [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-text-generation-inference-containers)\n",
    "\n",
    "_Note: At the time of writing this blog post the latest version of the Hugging Face LLM DLC is not yet available via the `get_huggingface_llm_image_uri` method. We are going to use the raw container uri instead._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1-tgi2.0-gpu-py310-cu121-ubuntu22.04\n"
     ]
    }
   ],
   "source": [
    "# COMMENT IN WHEN PR (https://github.com/aws/sagemaker-python-sdk/pull/4314) IS MERGED\n",
    "# from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# # retrieve the llm image uri\n",
    "# llm_image = get_huggingface_llm_image_uri(\n",
    "#   \"huggingface\",\n",
    "#   version=\"2.0.0\"\n",
    "# )\n",
    "llm_image = f\"763104351884.dkr.ecr.{sess.boto_region_name}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1-tgi2.0-gpu-py310-cu121-ubuntu22.04\"\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hardware requirements\n",
    "\n",
    "Llama 3 comes in 2 different sizes - 8B & 70B parameters. The hardware requirements will vary based on the model size deployed to SageMaker. Below is a set up minimum requirements for each model size we tested.\n",
    "\n",
    "| Model                                                              | Instance Type       | Quantization   | # of GPUs per replica |\n",
    "| ------------------------------------------------------------------ | ------------------- | -------------- | --------------------- |\n",
    "| [Llama 8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)   | `(ml.)g5.2xlarge`   | `-`            | 1                     |\n",
    "| [Llama 70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) | `(ml.)g5.12xlarge`  | `gptq` |Â `awq` | 8                     |\n",
    "| [Llama 70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) | `(ml.)g5.48xlarge`  | `-` | 8                     |\n",
    "| [Llama 70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) | `(ml.)p4d.24xlarge` | `-`            | 8                     |\n",
    "\n",
    "_Note: We haven't tested GPTQ or AWQ models yet._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy Llama 3 to Amazon SageMaker\n",
    "\n",
    "To deploy [Llama 3 70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) to Amazon SageMaker we create a `HuggingFaceModel` model class and define our endpoint configuration including the `hf_model_id`, `instance_type` etc. We will use a `p4d.24xlarge` instance type, which has 8 NVIDIA A100 GPUs and 320GB of GPU memory. Llama 3 70B instruct is a fine-tuned model for conversational AI this allows us to enable the [Messages API](https://huggingface.co/docs/text-generation-inference/messages_api) from TGI to interact with llama using the common OpenAI format of `messages`. \n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "    { \"role\": \"user\", \"content\": \"What is deep learning?\" }\n",
    "  ],\n",
    "}\n",
    "```\n",
    "\n",
    "_Note: Llama 3 is a gated model, please visit the [Model Card](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) and accept the license terms and acceptable use policy before submitting this form._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "health_check_timeout = 900\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"meta-llama/Meta-Llama-3-70B-Instruct\", # model_id from hf.co/models\n",
    "  'SM_NUM_GPUS': \"8\", # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': \"2048\",  # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': \"4096\",  # Max length of the generation (including input text)\n",
    "  'MAX_BATCH_TOTAL_TOKENS': \"8192\",  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "  'MESSAGES_API_ENABLED': \"true\", # Enable the messages API\n",
    "  'HUGGING_FACE_HUB_TOKEN': \"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\"\n",
    "}\n",
    "\n",
    "# check if token is set\n",
    "assert config['HUGGING_FACE_HUB_TOKEN'] != \"<REPLACE WITH YOUR TOKEN>\", \"Please set your Hugging Face Hub token\"\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created the `HuggingFaceModel` we can deploy it to Amazon SageMaker using the `deploy` method. We will deploy the model with the `ml.p4d.24xlarge` instance type. TGI will automatically distribute and shard the model across all GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------!"
     ]
    }
   ],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker will now create our endpoint and deploy the model to it. This can takes a 10-15 minutes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run inference and chat with the model\n",
    "\n",
    "After our endpoint is deployed we can run inference on it. We will use the `predict` method from the `predictor` to run inference on our endpoint. We can inference with different parameters to impact the generation. Parameters can be defined as in the `parameters` attribute of the payload. You can find supported parameters in the [here](https://huggingface.co/docs/text-generation-inference/messages_api). \n",
    "\n",
    "The Messages API allows us to interact with the model in a conversational way. We can define the role of the message and the content. The role can be either `system`,`assistant` or `user`. The `system` role is used to provide context to the model and the `user` role is used to ask questions or provide input to the model. \n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "    { \"role\": \"user\", \"content\": \"What is deep learning?\" }\n",
    "  ],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt to generate\n",
    "messages=[\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "    { \"role\": \"user\", \"content\": \"What is deep learning?\" }\n",
    "  ]\n",
    "\n",
    "# Generation arguments\n",
    "parameters = {\n",
    "    \"model\": \"meta-llama/Meta-Llama-3-70B-Instruct\", # placholder, needed\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 512,\n",
    "    \"stop\": [\"<|eot_id|>\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay lets test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning is a subset of machine learning that involves the use of artificial neural networks to model and solve complex problems. In a deep learning system, there are typically multiple layers of neural networks that process and transform the data in a hierarchical manner.\n",
      "\n",
      "The key characteristics of deep learning are:\n",
      "\n",
      "1. **Multiple layers**: Deep learning models have multiple layers of neural networks, which allow them to learn complex patterns and representations in data.\n",
      "2. **Hierarchical representations**: Each layer in a deep learning model builds on the previous layer, allowing the model to learn increasingly abstract and sophisticated representations of the data.\n",
      "3. **Automatic feature learning**: Deep learning models can automatically learn features and representations from the data, without the need for manual feature engineering.\n",
      "4. **Large amounts of data**: Deep learning models require large amounts of data to train, and they can learn to recognize patterns and make predictions based on that data.\n",
      "\n",
      "Some of the key benefits of deep learning include:\n",
      "\n",
      "1. **Improved accuracy**: Deep learning models can achieve state-of-the-art performance on a wide range of tasks, including image and speech recognition, natural language processing, and game playing.\n",
      "2. **Ability to handle large datasets**: Deep learning models can handle large amounts of data and learn to recognize patterns and make predictions based on that data.\n",
      "3. **Flexibility and adaptability**: Deep learning models can be used for a wide range of applications, and they can be adapted to new tasks and domains with relative ease.\n",
      "\n",
      "Some of the most popular deep learning techniques include:\n",
      "\n",
      "1. **Convolutional Neural Networks (CNNs)**: These are commonly used for image recognition and computer vision tasks.\n",
      "2. **Recurrent Neural Networks (RNNs)**: These are commonly used for natural language processing and speech recognition tasks.\n",
      "3. **Generative Adversarial Networks (GANs)**: These are commonly used for generating new data, such as images or music, that resembles existing data.\n",
      "\n",
      "Deep learning has many applications in areas such as:\n",
      "\n",
      "1. **Computer Vision**: Deep learning is used in applications such as image recognition, object detection, and facial recognition.\n",
      "2. **Natural Language Processing**: Deep learning is used in applications such as language translation, sentiment analysis, and chatbots.\n",
      "3. **Speech Recognition**: Deep learning is used in applications such as voice assistants and speech-to-text systems.\n",
      "4. **Robotics**: Deep learning is used in applications such as autonomous vehicles and robotic control systems.\n",
      "\n",
      "I hope this helps! Let me know if you have any further questions.\n"
     ]
    }
   ],
   "source": [
    "chat = llm.predict({\"messages\" :messages, **parameters})\n",
    "\n",
    "print(chat[\"choices\"][0][\"message\"][\"content\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Benchmark llama 3 70B\n",
    "\n",
    "We successfully deployed Llama 3 70B to Amazon SageMaker and tested it. Now we want to benchmark the model to see how it performs. We will use a [llmperf](https://github.com/philschmid/llmperf) fork with support for `sagemaker`. \n",
    "\n",
    "First lets install the `llmperf` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'llmperf' already exists and is not an empty directory.\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Obtaining file:///home/ubuntu/llm-sagemaker-sample/notebooks/llmperf\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pydantic<2.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from LLMPerf==0.1.0) (1.10.13)\n",
      "Requirement already satisfied: awscli>=1.22 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from LLMPerf==0.1.0) (1.32.84)\n",
      "Requirement already satisfied: typer>=0.4 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from LLMPerf==0.1.0) (0.6.1)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from LLMPerf==0.1.0) (4.36.2)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from LLMPerf==0.1.0) (4.66.2)\n",
      "Requirement already satisfied: litellm>=0.1.738 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from LLMPerf==0.1.0) (1.35.8)\n",
      "Requirement already satisfied: boto3 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from LLMPerf==0.1.0) (1.34.51)\n",
      "Requirement already satisfied: ray in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from LLMPerf==0.1.0) (2.10.0)\n",
      "Requirement already satisfied: num2words in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from LLMPerf==0.1.0) (0.5.13)\n",
      "Requirement already satisfied: pytest>=6.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from LLMPerf==0.1.0) (7.1.3)\n",
      "Requirement already satisfied: google-cloud-aiplatform in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from LLMPerf==0.1.0) (1.39.0)\n",
      "Requirement already satisfied: seaborn>=0.11 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from LLMPerf==0.1.0) (0.13.2)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from awscli>=1.22->LLMPerf==0.1.0) (4.7.2)\n",
      "Requirement already satisfied: botocore==1.34.84 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from awscli>=1.22->LLMPerf==0.1.0) (1.34.84)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from awscli>=1.22->LLMPerf==0.1.0) (0.10.0)\n",
      "Requirement already satisfied: PyYAML<6.1,>=3.10 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from awscli>=1.22->LLMPerf==0.1.0) (6.0.1)\n",
      "Requirement already satisfied: docutils<0.17,>=0.10 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from awscli>=1.22->LLMPerf==0.1.0) (0.16)\n",
      "Requirement already satisfied: colorama<0.4.5,>=0.2.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from awscli>=1.22->LLMPerf==0.1.0) (0.4.4)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from botocore==1.34.84->awscli>=1.22->LLMPerf==0.1.0) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from botocore==1.34.84->awscli>=1.22->LLMPerf==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from botocore==1.34.84->awscli>=1.22->LLMPerf==0.1.0) (1.26.18)\n",
      "Requirement already satisfied: tokenizers in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from litellm>=0.1.738->LLMPerf==0.1.0) (0.15.0)\n",
      "Requirement already satisfied: click in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from litellm>=0.1.738->LLMPerf==0.1.0) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from litellm>=0.1.738->LLMPerf==0.1.0) (6.11.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from litellm>=0.1.738->LLMPerf==0.1.0) (2.31.0)\n",
      "Requirement already satisfied: openai>=1.0.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from litellm>=0.1.738->LLMPerf==0.1.0) (1.19.0)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from litellm>=0.1.738->LLMPerf==0.1.0) (3.8.3)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from litellm>=0.1.738->LLMPerf==0.1.0) (3.1.2)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from litellm>=0.1.738->LLMPerf==0.1.0) (1.0.1)\n",
      "Requirement already satisfied: tiktoken>=0.4.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from litellm>=0.1.738->LLMPerf==0.1.0) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pydantic<2.5->LLMPerf==0.1.0) (4.7.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pytest>=6.0->LLMPerf==0.1.0) (1.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pytest>=6.0->LLMPerf==0.1.0) (23.2.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pytest>=6.0->LLMPerf==0.1.0) (2.0.1)\n",
      "Requirement already satisfied: iniconfig in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pytest>=6.0->LLMPerf==0.1.0) (1.1.1)\n",
      "Requirement already satisfied: py>=1.8.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pytest>=6.0->LLMPerf==0.1.0) (1.11.0)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pytest>=6.0->LLMPerf==0.1.0) (23.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from seaborn>=0.11->LLMPerf==0.1.0) (1.26.4)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from seaborn>=0.11->LLMPerf==0.1.0) (3.6.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from seaborn>=0.11->LLMPerf==0.1.0) (2.2.1)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-cloud-aiplatform->LLMPerf==0.1.0) (2.14.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-cloud-aiplatform->LLMPerf==0.1.0) (2.0.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-cloud-aiplatform->LLMPerf==0.1.0) (1.22.3)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-cloud-aiplatform->LLMPerf==0.1.0) (3.20.3)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-cloud-aiplatform->LLMPerf==0.1.0) (1.11.0)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-cloud-aiplatform->LLMPerf==0.1.0) (1.34.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-cloud-aiplatform->LLMPerf==0.1.0) (3.16.0)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from num2words->LLMPerf==0.1.0) (0.6.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from ray->LLMPerf==0.1.0) (1.0.4)\n",
      "Requirement already satisfied: frozenlist in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from ray->LLMPerf==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from ray->LLMPerf==0.1.0) (3.12.2)\n",
      "Requirement already satisfied: jsonschema in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from ray->LLMPerf==0.1.0) (4.21.1)\n",
      "Requirement already satisfied: aiosignal in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from ray->LLMPerf==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from transformers->LLMPerf==0.1.0) (0.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from transformers->LLMPerf==0.1.0) (0.21.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from transformers->LLMPerf==0.1.0) (2022.9.13)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->LLMPerf==0.1.0) (1.58.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->LLMPerf==0.1.0) (2.26.2)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->LLMPerf==0.1.0) (1.48.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->LLMPerf==0.1.0) (1.50.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform->LLMPerf==0.1.0) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform->LLMPerf==0.1.0) (2.7.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform->LLMPerf==0.1.0) (0.13.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform->LLMPerf==0.1.0) (1.5.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->LLMPerf==0.1.0) (2023.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from importlib-metadata>=6.8.0->litellm>=0.1.738->LLMPerf==0.1.0) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from jinja2<4.0.0,>=3.1.2->litellm>=0.1.738->LLMPerf==0.1.0) (2.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.11->LLMPerf==0.1.0) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.11->LLMPerf==0.1.0) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.11->LLMPerf==0.1.0) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.11->LLMPerf==0.1.0) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.11->LLMPerf==0.1.0) (9.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.11->LLMPerf==0.1.0) (1.0.6)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from openai>=1.0.0->litellm>=0.1.738->LLMPerf==0.1.0) (1.8.0)\n",
      "Requirement already satisfied: sniffio in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from openai>=1.0.0->litellm>=0.1.738->LLMPerf==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from openai>=1.0.0->litellm>=0.1.738->LLMPerf==0.1.0) (3.6.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from openai>=1.0.0->litellm>=0.1.738->LLMPerf==0.1.0) (0.23.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pandas>=1.2->seaborn>=0.11->LLMPerf==0.1.0) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pandas>=1.2->seaborn>=0.11->LLMPerf==0.1.0) (2024.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from requests<3.0.0,>=2.31.0->litellm>=0.1.738->LLMPerf==0.1.0) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from requests<3.0.0,>=2.31.0->litellm>=0.1.738->LLMPerf==0.1.0) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from requests<3.0.0,>=2.31.0->litellm>=0.1.738->LLMPerf==0.1.0) (2.1.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from rsa<4.8,>=3.1.2->awscli>=1.22->LLMPerf==0.1.0) (0.4.8)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from aiohttp->litellm>=0.1.738->LLMPerf==0.1.0) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from aiohttp->litellm>=0.1.738->LLMPerf==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from aiohttp->litellm>=0.1.738->LLMPerf==0.1.0) (1.8.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from jsonschema->ray->LLMPerf==0.1.0) (2023.12.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from jsonschema->ray->LLMPerf==0.1.0) (0.18.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from jsonschema->ray->LLMPerf==0.1.0) (0.33.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->LLMPerf==0.1.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->LLMPerf==0.1.0) (5.2.0)\n",
      "Requirement already satisfied: six>=1.5.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from grpcio<2.0dev,>=1.33.2->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->LLMPerf==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: httpcore<0.16.0,>=0.15.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai>=1.0.0->litellm>=0.1.738->LLMPerf==0.1.0) (0.15.0)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai>=1.0.0->litellm>=0.1.738->LLMPerf==0.1.0) (1.5.0)\n",
      "Requirement already satisfied: h11<0.13,>=0.11 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from httpcore<0.16.0,>=0.15.0->httpx<1,>=0.23.0->openai>=1.0.0->litellm>=0.1.738->LLMPerf==0.1.0) (0.12.0)\n",
      "Building wheels for collected packages: LLMPerf\n",
      "  Building editable for LLMPerf (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for LLMPerf: filename=LLMPerf-0.1.0-0.editable-py3-none-any.whl size=6152 sha256=3e16554a1c733cb1879158d7aeb9f659b19b42c220e111c4e2d9d589549d2d1d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-butgec4c/wheels/9b/1e/39/5584e6682f0d0f1a03a05454903fa3ae7fa095b8e3890182c4\n",
      "Successfully built LLMPerf\n",
      "Installing collected packages: LLMPerf\n",
      "  Attempting uninstall: LLMPerf\n",
      "    Found existing installation: LLMPerf 0.1.0\n",
      "    Uninstalling LLMPerf-0.1.0:\n",
      "      Successfully uninstalled LLMPerf-0.1.0\n",
      "Successfully installed LLMPerf-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/philschmid/llmperf.git \n",
    "!pip install -e llmperf/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the benchmark with the following command. We are going to benchmark using `25` concurrent users and max `500` requests. The benchmark will measure `first-time-to-token`, `latency (ms/token)` and `throughput (tokens/s)` full details can be found in the `results` folder\n",
    "\n",
    "_ð¨Importantð¨: This benchmark was initiatied from Europe, while the endpoint runs in us-east-1. This has significant impact on the `first-time-to-token` metric, since it includes the network communication. If you want to measure the `first-time-to-token` correctly, you need to run the benchmark on the same host or your production region._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-18 19:31:16,970\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "  0%|                                                   | 0/500 [00:03<?, ?it/s]\u001b[36m(SageMakerClient pid=7215)\u001b[0m No AWS_ACCESS_KEY_ID found in the environment. Use the default AWS credentials.\n",
      "\u001b[36m(SageMakerClient pid=7215)\u001b[0m No AWS_SECRET_ACCESS_KEY found in the environment. Use the default AWS credentials.\n",
      "\u001b[36m(SageMakerClient pid=7215)\u001b[0m No AWS_REGION found in the environment. Use the default AWS credentials.\n",
      "  5%|ââ                                        | 25/500 [00:16<03:36,  2.20it/s]\u001b[36m(SageMakerClient pid=7928)\u001b[0m No AWS_ACCESS_KEY_ID found in the environment. Use the default AWS credentials.\u001b[32m [repeated 25x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(SageMakerClient pid=7928)\u001b[0m No AWS_SECRET_ACCESS_KEY found in the environment. Use the default AWS credentials.\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "\u001b[36m(SageMakerClient pid=7928)\u001b[0m No AWS_REGION found in the environment. Use the default AWS credentials.\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      " 10%|âââââ                                     | 50/500 [00:31<03:57,  1.90it/s]\u001b[36m(SageMakerClient pid=7217)\u001b[0m No AWS_ACCESS_KEY_ID found in the environment. Use the default AWS credentials.\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "\u001b[36m(SageMakerClient pid=7217)\u001b[0m No AWS_SECRET_ACCESS_KEY found in the environment. Use the default AWS credentials.\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "\u001b[36m(SageMakerClient pid=7217)\u001b[0m No AWS_REGION found in the environment. Use the default AWS credentials.\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      " 15%|âââââââ                                   | 75/500 [00:44<03:41,  1.92it/s]\u001b[36m(SageMakerClient pid=7210)\u001b[0m No AWS_ACCESS_KEY_ID found in the environment. Use the default AWS credentials.\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "\u001b[36m(SageMakerClient pid=7210)\u001b[0m No AWS_SECRET_ACCESS_KEY found in the environment. Use the default AWS credentials.\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "\u001b[36m(SageMakerClient pid=7210)\u001b[0m No AWS_REGION found in the environment. Use the default AWS credentials.\u001b[32m [repeated 25x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# tell llmperf that we are using the messages api\n",
    "!MESSAGES_API=true python llmperf/token_benchmark_ray.py \\\n",
    "--model {llm.endpoint_name} \\\n",
    "--llm-api \"sagemaker\" \\\n",
    "--max-num-completed-requests 500 \\\n",
    "--timeout 600 \\\n",
    "--num-concurrent-requests 25 \\\n",
    "--results-dir \"results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets parse the results and display them nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concurrent requests: 25\n",
      "Avg. Input token length: 550\n",
      "Avg. Output token length: 150\n",
      "Avg. First-Time-To-Token: 1301.28ms\n",
      "Avg. Thorughput: 1116.25 tokens/sec\n",
      "Avg. Latency: 10.45ms/token\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "# Reads the summary.json file and prints the results\n",
    "with open(glob.glob(f'results/*summary.json')[0], 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(\"Concurrent requests: 25\")\n",
    "print(f\"Avg. Input token length: {data['mean_input_tokens']}\")\n",
    "print(f\"Avg. Output token length: {data['mean_output_tokens']}\")\n",
    "print(f\"Avg. First-Time-To-Token: {data['results_ttft_s_mean']*1000:.2f}ms\")\n",
    "print(f\"Avg. Thorughput: {data['results_mean_output_throughput_token_per_s']:.2f} tokens/sec\")\n",
    "print(f\"Avg. Latency: {data['results_inter_token_latency_s_mean']*1000:.2f}ms/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats it! We successfully deployed, tested and benchmarked Llama 3 70B on Amazon SageMaker. The benchmark is not a full representation of the model performance, but it gives you a first good indication. If you plan to use the model in production, we recommend to run a longer and closer to your production benchmark, modify the number of replicas see ([Scale LLM Inference on Amazon SageMaker with Multi-Replica Endpoints](https://www.philschmid.de/sagemaker-multi-replicahttps://www.philschmid.de/sagemaker-multi-replica)) and most importantly test the model with your own data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clean up\n",
    "\n",
    "To clean up, we can delete the model and endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fcf248a74081676ead7e77f54b2c239ba2921b952f7cbcdbbe5427323165924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
